{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM9zaeDDnerC7TL/yWvduDn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JeanPinhata/PROGRAMA-O-DE-LINGUAGEM-NATURAL-/blob/main/EXTRA%C3%87%C3%83O_DE_ENTIDADES_NOMEADAS_COM_LLM_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets seqeval scikit-learn\n",
        "!pip install --upgrade transformers\n",
        "\n",
        "import requests\n",
        "import re\n",
        "from io import StringIO\n",
        "import pandas as pd\n",
        "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer, AutoTokenizer, pipeline\n",
        "from seqeval.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Ucl5T7P06ubs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Baixar dados\n",
        "url = \"https://raw.githubusercontent.com/davidsbatista/NER-datasets/refs/heads/master/Portuguese/leNER-Br/dev.conll\"\n",
        "response = requests.get(url)\n",
        "raw_text = response.text\n",
        "\n",
        "# Parse\n",
        "sentences = []\n",
        "ner_tags = []\n",
        "tokens = []\n",
        "tags = []\n",
        "\n",
        "for line in raw_text.splitlines():\n",
        "    line = line.strip()\n",
        "    if line == \"\":\n",
        "        if tokens:\n",
        "            sentences.append(tokens)\n",
        "            ner_tags.append(tags)\n",
        "            tokens, tags = [], []\n",
        "    else:\n",
        "        parts = re.split(r\"\\s+\", line)\n",
        "        if len(parts) == 2:\n",
        "            token, tag = parts\n",
        "            tokens.append(token)\n",
        "            tags.append(tag)\n",
        "\n",
        "if tokens:\n",
        "    sentences.append(tokens)\n",
        "    ner_tags.append(tags)\n",
        "\n",
        "examples = [{\"tokens\": s, \"ner_tags\": t} for s, t in zip(sentences, ner_tags)]\n",
        "\n",
        "print(f\"Total de sentenças: {len(examples)}\")\n",
        "print(\"Exemplo:\", examples[0])\n"
      ],
      "metadata": {
        "id": "hU49_M8A-Akt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "train_data, test_data = train_test_split(examples, test_size=0.2, random_state=42)\n",
        "\n",
        "dataset = DatasetDict({\n",
        "    \"train\": Dataset.from_list(train_data),\n",
        "    \"test\": Dataset.from_list(test_data),\n",
        "})\n"
      ],
      "metadata": {
        "id": "JEvGeIeV_hcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoint = \"neuralmind/bert-base-portuguese-cased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "J083Ox3NKTwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")\n",
        "\n",
        "# Mapear rótulos\n",
        "label_list = sorted(list({tag for ex in examples for tag in ex[\"ner_tags\"]}))\n",
        "label2id = {label: i for i, label in enumerate(label_list)}\n",
        "id2label = {i: label for label, i in label2id.items()}\n",
        "\n",
        "# Função de tokenização\n",
        "def tokenize_and_align_labels(example):\n",
        "    tokenized_inputs = tokenizer(\n",
        "        example[\"tokens\"],\n",
        "        is_split_into_words=True,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",  # <- importante!\n",
        "        max_length=128,        # <- para uniformizar o tamanho\n",
        "    )\n",
        "\n",
        "    word_ids = tokenized_inputs.word_ids()\n",
        "    label_ids = []\n",
        "    previous_word_idx = None\n",
        "    for word_idx in word_ids:\n",
        "        if word_idx is None:\n",
        "            label_ids.append(-100)\n",
        "        elif word_idx != previous_word_idx:\n",
        "            label_ids.append(label2id[example[\"ner_tags\"][word_idx]])\n",
        "        else:\n",
        "            label_ids.append(-100)  # sub-palavras são ignoradas\n",
        "        previous_word_idx = word_idx\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = label_ids\n",
        "    return tokenized_inputs\n",
        "\n",
        "# Tokenizar dataset\n",
        "tokenized_dataset = dataset.map(tokenize_and_align_labels)\n",
        "print(\"Entidades reconhecidas:\", label_list)"
      ],
      "metadata": {
        "id": "z-o9yTnH_mao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    \"neuralmind/bert-base-portuguese-cased\",\n",
        "    num_labels=len(label_list),\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./ner-bert-learnbr\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"no\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=10,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "# Métricas\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "    true_preds = [\n",
        "        [id2label[p] for (p, l) in zip(pred, lab) if l != -100]\n",
        "        for pred, lab in zip(predictions, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [id2label[l] for (p, l) in zip(pred, lab) if l != -100]\n",
        "        for pred, lab in zip(predictions, labels)\n",
        "    ]\n",
        "    return {\n",
        "        \"precision\": precision_score(true_labels, true_preds),\n",
        "        \"recall\": recall_score(true_labels, true_preds),\n",
        "        \"f1\": f1_score(true_labels, true_preds),\n",
        "        \"accuracy\": accuracy_score(true_labels, true_preds),\n",
        "    }\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "qvgMgHxj_qOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = list(range(1, 11))\n",
        "f1_scores = [\n",
        "    0.547731,\n",
        "    0.788991,\n",
        "    0.849558,\n",
        "    0.876336,\n",
        "    0.891975,\n",
        "    0.888550,\n",
        "    0.892638,\n",
        "    0.888208,\n",
        "    0.902628,\n",
        "    0.895385\n",
        "]\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(epochs, f1_scores, marker='o', linestyle='-', color='skyblue')\n",
        "plt.title(\"Evolução do F1-score por Época\")\n",
        "plt.xlabel(\"Época\")\n",
        "plt.ylabel(\"F1-score\")\n",
        "plt.xticks(epochs)\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "25uvl1ydKdWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "epochs = list(range(1, 11))\n",
        "\n",
        "precision = [\n",
        "    0.501377,\n",
        "    0.789318,\n",
        "    0.858859,\n",
        "    0.863636,\n",
        "    0.872340,\n",
        "    0.884848,\n",
        "    0.886850,\n",
        "    0.875000,\n",
        "    0.902141,\n",
        "    0.914373\n",
        "]\n",
        "\n",
        "recall = [\n",
        "    0.568750,\n",
        "    0.831250,\n",
        "    0.893750,\n",
        "    0.890625,\n",
        "    0.896875,\n",
        "    0.912500,\n",
        "    0.906250,\n",
        "    0.896875,\n",
        "    0.921875,\n",
        "    0.934375\n",
        "]\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "ax1 = axes[0]\n",
        "ax1.plot(epochs, precision, marker='o', linestyle='-', color='blue')\n",
        "ax1.set_title(\"Evolução da Precisão por Época\")\n",
        "ax1.set_xlabel(\"Época\")\n",
        "ax1.set_ylabel(\"Precision Score\")\n",
        "ax1.set_ylim(0.4, 1.0)\n",
        "ax1.set_xticks(epochs)\n",
        "ax1.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "ax2 = axes[1]\n",
        "ax2.plot(epochs, recall, marker='x', linestyle='-', color='red')\n",
        "ax2.set_title(\"Evolução do Recall por Época\")\n",
        "ax2.set_xlabel(\"Época\")\n",
        "ax2.set_ylabel(\"Recall Score\")\n",
        "ax2.set_ylim(0.4, 1.0)\n",
        "ax2.set_xticks(epochs)\n",
        "ax2.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "_HNUGnAuKdUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"bert-lenerbr-final-f1-0_92\")\n"
      ],
      "metadata": {
        "id": "HRI_r5QlL50d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"/content/bert-lenerbr-final-f1-0_92\"\n",
        "\n",
        "if not os.path.exists(model_path):\n",
        "    print(f\"Erro: O diretório do modelo '{model_path}' não foi encontrado.\")\n",
        "    print(\"Por favor, verifique o caminho e certifique-se de que o modelo foi salvo corretamente.\")\n",
        "else:\n",
        "    print(f\"Carregando modelo do caminho: {model_path}\")\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "        ner_pipeline = pipeline(\n",
        "            \"ner\",\n",
        "            model=model_path,\n",
        "            tokenizer=tokenizer,\n",
        "            aggregation_strategy=\"simple\"\n",
        "        )\n",
        "        print(\"Modelo e pipeline carregados com sucesso!\")\n",
        "\n",
        "        text = \"Em 18 de julho de 2023, a advogada Ana Paula Resende da Ordem dos Advogados do Brasil, localizada em Curitiba, argumentou com base no Artigo 14 da Constituição Federal e citou o Recurso Especial 123.456/SP, reforçando seu ponto.\"\n",
        "\n",
        "        try:\n",
        "            entities = ner_pipeline(text)\n",
        "\n",
        "            if entities:\n",
        "                print(\"\\nEntidades Extraídas:\")\n",
        "                for entity in entities:\n",
        "                    print(f\"'{entity['word']}' -> Tipo: {entity['entity_group']}\")\n",
        "            else:\n",
        "                print(\"Nenhuma entidade foi encontrada nesta frase.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Ocorreu um erro ao processar a frase: {e}\")\n",
        "            print(\"Por favor, tente novamente ou verifique se o modelo foi carregado corretamente.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao carregar o modelo ou pipeline: {e}\")\n",
        "        print(\"Verifique se as bibliotecas estão instaladas e se o modelo no caminho especificado é válido.\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Gt_YN2TVKqd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, AutoTokenizer\n",
        "import os\n",
        "\n",
        "# --- 1. Caminho para o modelo salvo (ajuste conforme seu diretório) ---\n",
        "model_path = \"/content/bert-lenerbr-final-f1-0_92\"\n",
        "\n",
        "if not os.path.exists(model_path):\n",
        "    print(f\"Erro: O diretório do modelo '{model_path}' não foi encontrado.\")\n",
        "    print(\"Por favor, verifique o caminho e certifique-se de que o modelo foi salvo corretamente.\")\n",
        "else:\n",
        "    print(f\"Carregando modelo do caminho: {model_path}\")\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "        ner_pipeline = pipeline(\n",
        "            \"ner\",\n",
        "            model=model_path,\n",
        "            tokenizer=tokenizer,\n",
        "            aggregation_strategy=\"simple\"\n",
        "        )\n",
        "        print(\"Modelo e pipeline carregados com sucesso!\")\n",
        "\n",
        "        # --- 3. Loop para entrada do usuário ---\n",
        "        print(\"\\n--- Modo Interativo NER ---\")\n",
        "        print(\"Digite uma frase para extrair entidades. Digite 'sair' para encerrar.\")\n",
        "\n",
        "        while True:\n",
        "            text = input(\"\\nSua frase: \")\n",
        "            if text.lower() == 'sair':\n",
        "                print(\"Encerrando o programa.\")\n",
        "                break\n",
        "            if not text.strip():\n",
        "                print(\"Por favor, digite uma frase válida.\")\n",
        "                continue\n",
        "\n",
        "            # --- 4. Executar o modelo ---\n",
        "            try:\n",
        "                entities = ner_pipeline(text)\n",
        "\n",
        "                # --- 5. Exibir as entidades extraídas (sem o score) ---\n",
        "                if entities:\n",
        "                    print(\"\\nEntidades Extraídas:\")\n",
        "                    for entity in entities:\n",
        "                        print(f\"'{entity['word']}' -> Tipo: {entity['entity_group']}\")\n",
        "                else:\n",
        "                    print(\"Nenhuma entidade foi encontrada nesta frase.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Ocorreu um erro ao processar a frase: {e}\")\n",
        "                print(\"Por favor, tente novamente ou verifique se o modelo foi carregado corretamente.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao carregar o modelo ou pipeline: {e}\")\n",
        "        print(\"Verifique se as bibliotecas estão instaladas e se o modelo no caminho especificado é válido.\")\n",
        "\n",
        "  #Em 18 de julho de 2023, a advogada Ana Paula Resende da Ordem dos Advogados do Brasil, localizada em Curitiba, argumentou com base no Artigo 14 da Constituição Federal e citou o Recurso Especial 123.456/SP, reforçando seu ponto.#"
      ],
      "metadata": {
        "id": "EFWWc7EBGcVt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}